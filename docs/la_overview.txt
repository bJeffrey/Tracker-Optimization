Linear Algebra Backend Abstraction Overview
===========================================

This project demonstrates a high‑performance, backend‑agnostic linear algebra
framework designed for large‑scale batch covariance updates (typical in radar,
ESM, tracking, and filtering systems). The core objective is to **abstract the
matrix backend implementation**—Eigen, Intel MKL, or Standard C++—behind a
unified API, while allowing the build system to select the optimal backend
depending on hardware, user choice, or offline constraints.

Core Concepts
-------------

1. Unified API (`la.h`)
   - Defines backend‑independent interfaces such as:
       * GEMM
       * Covariance prediction
       * Batch random‑walk updates
       * Symmetrization
   - Every backend implements the same functions.

2. Backend Implementations
   - `la_eigen.cpp`  
     Uses Eigen3 with row‑major matrices, SIMD vectorization, and optional
     OpenMP acceleration. Great general‑purpose performance across CPUs.

   - `la_mkl.cpp`  
     Uses Intel oneAPI MKL for BLAS/LAPACK operations. Extremely fast on
     Intel CPUs and optimized for large matrices and multithreading.

   - `la_std.cpp`  
     Pure C++17 implementation without external dependencies. Excellent for
     offline systems or controlled deployments. Optimized loop ordering and
     cache friendliness make it competitive for small 6×6 operations.

3. Batch‑Optimized Design
   - Covariance matrices are stored in contiguous blocks.
   - Minimal pointer chasing and fully row‑major access.
   - When F = I, the code uses the **fast path**:
       P_i ← P_i + Q*dt
     avoiding two redundant matrix multiplies.

4. Build System & Selection
   - CMake selects backend via:
       * `-DUSE_MKL=ON`
       * `-DUSE_STD=ON`
       * default = Eigen
   - Smart script (`build.sh`) auto‑detects Intel CPUs and chooses MKL.

5. Containers (CentOS Stream 9)
   - app‑eigen.Dockerfile
   - app‑mkl.Dockerfile
   - app‑std.Dockerfile
   Each builds a self‑contained runtime image.

Performance
-----------
Across 100,000‑matrix batch updates (6×6 covariances):

- MKL: Fastest (0.001–0.01 s typical)
- Eigen: Consistent, mid‑range performance (0.01–0.02 s)
- STD: Surprisingly strong for small matrices (0.01–0.02 s)

The abstraction ensures all three produce identical numerical results.

Purpose
-------
This design supports:
- Radar & ESM covariance propagation
- Tracking filters (KF, EKF, UKF)
- Reinforcement‑learning simulators
- Embedded or offline systems without external libraries

And most importantly—it enables **backend swapping without touching user code**.

