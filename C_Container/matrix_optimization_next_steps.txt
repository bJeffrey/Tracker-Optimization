Matrix Optimization – Next Steps (Post Config-Driven Demo, Sensor-Driven Scan Queries)

Checkpoint (what’s working now)
- XML + XSD validation works end-to-end
- Config loader resolves all referenced files correctly (system → scenario → ownship/targets)
- Demo runs a backend-selected batched RW covariance update (P += Q·dt) using full 9×9
- Timing + checksum output is stable
- Sensors.xml defines a scan frustum + schedule that can drive “tracks-in-scan-volume” queries

Recommended next steps (minimal scope, maximum payoff)

1) Add a TrackBatch structure + metadata arrays (no database yet)
Goal: keep your fast SoA batch layout, but start carrying the minimal “real tracker” metadata you’ll need.
Suggested fields (SoA arrays):
- track_id[]            (uint64)
- last_update_time_s[]  (double)
- status[]              (enum: ACTIVE / COASTING / DROPPED)
- quality[]             (float, optional)
- x[]                   (state, batch-major: N x 9 doubles; ECEF pos/vel/acc)
- P[]                   (covariance, batch-major: N x 81 doubles; full 9×9 for now)

Why:
- Enables hit/miss, aging, scheduling policy, and later DB interactions without changing memory layout.
- Preserves contiguous buffers for batched math kernels.

2) Generate synthetic tracks deterministically (ECEF) from config
Goal: replace “load from DB” with deterministic “seeded generation” so performance tests are repeatable.
Use TargetsGen config:
- Position: ECEF (use ecef_box mixture for both Background and Local, as you validated)
- Velocity/Acceleration: distributions from config
- Covariance initialization: sigma ranges or diagonal (start simple)
- last_update_time_s: use model (uniform_age / exponential_age / mixture_age)

Why:
- Lets you scale quickly to 10k–1M tracks reproducibly.
- Produces the variability needed for update scheduling and later association testing.

3) Add a sensor-driven scan query planner (broadphase) that produces ECEF AABBs
Goal: query tracks by where the sensor will be scanning next (driven by sensors.xml), without scanning the full DB.
From sensors.xml, derive a lookahead set of dwells:
- ScanRateHz → dwell period Tscan
- For each dwell time t_scan:
  - Use ownship ECEF position and attitude (R_ecef_from_body)
  - Interpret scan frustum in OWNERSHIP_BODY (az/el/range min/max)
  - Convert the frustum to a conservative ECEF AABB by sampling corner rays at r_min/r_max
  - Inflate the AABB by QueryAabbInflateMeters
Output per dwell:
- t_scan
- AABB in ECEF (minX/maxX/minY/maxY/minZ/maxZ)
- (retain frustum params for fine filtering later)

Why:
- Broadphase query uses simple AABB overlap, which maps directly to grid hashing and SQLite R*Tree.
- Keeps “sensor geometry math” out of the DB query stage.

4) Implement broadphase→narrowphase subset processing using the planned scan AABB(s)
Goal: validate the core runtime pattern you’ll reuse with a real DB/index.
For each dwell (or union of a few dwells):
- candidates = index.query(AABB_ecef)
- gather: build contiguous x_batch / P_batch for candidate indices
- propagate: run RW batched update (current fast kernel) to t_scan or dt
- narrowphase filter (fine geometry):
  - for candidates, compute relative vector and az/el/range in OWNERSHIP_BODY
  - keep only those truly inside frustum
- scatter: write updated x/P back to the global arrays

Why:
- This is the speed-critical pattern: AABB query → contiguous compute → scatter.
- Narrowphase conversion to AER/az-el happens only on candidates, not on the full track set.

5) Add timing breakdowns so optimization is guided by data
Goal: see where time goes before expanding scope.
Record timing for:
- scan planner (AABB construction per dwell)
- broadphase query (index/DB)
- gather
- propagate
- narrowphase filter
- scatter
- total

Why:
- Helps you decide if the next investment should be better indexing, better cache locality, larger batches, or parallel strategy.

Next deliverable (once you’re ready)
- Patch zip that adds:
  - TrackBatch (SoA)
  - ScanVolumePlanner (reads sensors.xml and emits ECEF AABBs per dwell)
  - A simple broadphase index (grid hash) that can later be swapped for SQLite R*Tree
  - main.cpp flow: generate→index→plan scans→query→gather→propagate→filter→scatter
  - timing breakdown prints
